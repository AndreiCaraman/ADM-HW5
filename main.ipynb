{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e40a805-b4c8-4c5f-90e5-f34c40984800",
   "metadata": {},
   "source": [
    "# Exploring StackOverflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633c5809-17a8-4c32-b86c-f4d1219bd16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13428b8f-87cc-4e27-860b-3e3d044ed6dc",
   "metadata": {},
   "source": [
    "## Populating the Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f8e26-6488-48a3-b8eb-af1da366985c",
   "metadata": {},
   "source": [
    "Each graph is build independantly from the provided `.txt` files of temporal network of interactions. \\\n",
    "Users are represented as nodes and answers\\comments as edges.\n",
    "\n",
    "The design choices of the following method are:\n",
    "- Using directed graphs. \n",
    "- *Simple* graphs, there are no loops in the graphs. Users who answer to themselves are discarded cases.\n",
    "- Only one attributes is assigned to the edges: weight. The weights are:\n",
    "    - `1` for answers to questions\n",
    "    - `2/3` for comments on questions\n",
    "    - `1/2` for comments to answers\n",
    "- Time resolution is one day.\n",
    "- The graphs are build given a time input to avoid such attribute for the sake of simplicity and robustess.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcb35e9-e745-4a97-a9ca-c989189dcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_time_int_to_dates(time_interval):\n",
    "    # Convert the time interval in start and end dates \n",
    "    time_interval = tuple(map(datetime.fromisoformat, time_interval)) # converting time interval into datetime format\n",
    "    time_interval = tuple(map(datetime.timestamp, time_interval)) # converting time interval into POSIX timestamp \n",
    "    start_d = int(time_interval[0]) #converting to string to compare with the txt\n",
    "    end_d = int(time_interval[1])\n",
    "    return start_d, end_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "01fa146e-6614-4a5c-89eb-a60358889e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_graph(file):\n",
    "    \n",
    "    # Initialize the directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Create mapping of files\n",
    "    map_files = {1: \"sx-stackoverflow-a2q.txt\", 2:\"sx-stackoverflow-c2q.txt\", 3:\"sx-stackoverflow-c2a.txt\"}\n",
    "    \n",
    "    # Select the file chosen, open it and read the lines\n",
    "    with open(map_files[file], \"r\", encoding=\"UTF-8\") as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            \n",
    "            # Parse the line\n",
    "            elems = line.split(' ')\n",
    "            \n",
    "            # Add the edge to the graph if it is not present\n",
    "            if (elems[0], elems[1]) not in G.edges():\n",
    "                G.add_edge(elems[0], elems[1])\n",
    "                    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abbfe1-310e-4171-9be1-b6a2434662e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1_full = get_full_graph(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d85cf-7dc1-48e0-a192-e24fdf55fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph2_full = get_full_graph(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e601e7-7b7e-4164-884a-a2a0410d41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph3_full = get_full_graph(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea591c00-2fd7-4634-8e76-aa983b054401",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph1_full)\n",
    "print(graph2_full)\n",
    "print(graph3_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc3c5fb-5b1e-44c1-b23e-b6b0728d9a44",
   "metadata": {},
   "source": [
    "#### Write the 3 full graphs into files to save computation each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb4265-d53d-4d27-8571-1f8623640ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(graph1_full, \"graph1_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d9e4e-7f41-4b8e-a75c-3264dd2f5d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(graph2_full, \"graph2_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53784624-95e9-4b15-a272-5c17e091e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(graph3_full, \"graph3_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63742a0b-fd36-4aef-87aa-37c561ae9dde",
   "metadata": {},
   "source": [
    "#### Read the 3 full graphs from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032d007-7ac5-4f4b-9ba2-4ce2c5dd3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.read_gml(\"graph1_full\")\n",
    "nx.read_gml(\"graph2_full\")\n",
    "nx.read_gml(\"graph3_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f87aa2-c771-40a2-9064-4b2f2444d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(time_interval, file = 3):\n",
    "    \n",
    "    # Initialize the graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Create mapping of files and mapping of weights\n",
    "    map_files = {1: \"sx-stackoverflow-a2q.txt\", 2:\"sx-stackoverflow-c2q.txt\", 3:\"sx-stackoverflow-c2a.txt\"}\n",
    "    map_weights = {1: 1.0, 2: 2/3, 3: 1/2}\n",
    "    \n",
    "    # Get the start and end dates \n",
    "    start, end = from_time_int_to_dates(time_interval)\n",
    "    \n",
    "    # Select the file chosen, open it and read the lines\n",
    "    with open(map_files[file], \"r\", encoding=\"UTF-8\") as f:\n",
    "        for line in tqdm(f.readlines()):\n",
    "            \n",
    "            # Parse the line\n",
    "            elems = line.split(' ')\n",
    "            \n",
    "            # Add to the graph if it is in the time interval\n",
    "            if start <= int(elems[2]) <= end:\n",
    "                # If the edge already exists --> increment the weight, else simply add the new edge\n",
    "                if (elems[0], elems[1]) in G.edges():\n",
    "                    G[elems[0]][elems[1]]['weight'] += float(map_weights[file])\n",
    "                else:\n",
    "                    G.add_edge(elems[0], elems[1], weight = float(map_weights[file]))\n",
    "                    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a9c6c1-f32e-4670-ad92-5bd0bdff80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = (\"2008-11-01\",\"2008-11-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a30b3d-9ea6-4231-9f3e-92a8df1db28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17823525/17823525 [00:40<00:00, 441142.66it/s]\n"
     ]
    }
   ],
   "source": [
    "graph1 = get_graph(time_interval=interval, file=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba3bc214-1db1-4c99-b8c7-110519bb3449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20268151/20268151 [00:51<00:00, 394643.74it/s]\n"
     ]
    }
   ],
   "source": [
    "graph2 = get_graph(time_interval=interval, file=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "714d350c-7ab9-42aa-9223-15c6f0a7db31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25405374/25405374 [01:09<00:00, 363701.02it/s]\n"
     ]
    }
   ],
   "source": [
    "graph3 = get_graph(time_interval=interval, file=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89c58c22-a182-4231-b5e7-79516bc2289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 4668 nodes and 10758 edges\n",
      "DiGraph with 1111 nodes and 1177 edges\n",
      "DiGraph with 3070 nodes and 5990 edges\n"
     ]
    }
   ],
   "source": [
    "print(graph1)\n",
    "print(graph2)\n",
    "print(graph3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2316458-1e10-4fa5-8775-96ac64a07bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = nx.to_dict_of_dicts(graph1)\n",
    "d2 = nx.to_dict_of_dicts(graph2)\n",
    "d3 = nx.to_dict_of_dicts(graph3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e418e61-18ae-4ca3-b099-e2162127e3a2",
   "metadata": {},
   "source": [
    "### Merging the graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8c473-b071-4a5a-8f01-d0597e1ff29e",
   "metadata": {},
   "source": [
    "This function will merge two graphs that were obtained by the function **get_graph()** with the same time interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dea53ebc-8ba8-4beb-ba6c-fe2985e52564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_graph(graph_1, graph_2):\n",
    "    \n",
    "    # Iterate over the edges from the second graph\n",
    "    for edge_2 in graph_2.edges(data = True):\n",
    "        # If the edge of graph 2 is also in graph 1, only sum weights\n",
    "        if (edge_2[0],edge_2[1]) in graph_1.edges():\n",
    "            graph_1[edge_2[0]][edge_2[1]]['weight'] += float(edge_2[2]['weight'])\n",
    "        # Else add the edge of graph 2 also in graph 1\n",
    "        else:\n",
    "            graph_1.add_edge(edge_2[0], edge_2[1], weight = float(edge_2[2]['weight']))\n",
    "            \n",
    "    return graph_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42cc4687-2d01-48f7-90e8-e9adfa2301ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged_graph(graph1, graph2)\n",
    "merged = merged_graph(merged, graph3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "298b0936-2f34-4950-9a99-daf15e2c95c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 5134 nodes and 17414 edges\n"
     ]
    }
   ],
   "source": [
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32daafc8-c3ff-4bc1-8a66-29d7cbbc104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_merge = nx.to_dict_of_dicts(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3198d9-4c5d-49db-a906-8dbff551a931",
   "metadata": {},
   "source": [
    "### Functionality 1 - Get the overall features of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52219954-599c-451e-b611-805763fb621b",
   "metadata": {},
   "source": [
    "Graph density/sprcity is computed as defined in *\"Introduction to Algorithms\" by Cormern, Leiserson, Rivest, and Stein*: \\\n",
    "A graph $G = (E, V)$, with $E$ denoting the edges and $V$ denoting the vertices,  is sparse if $|E| << |V|^2$ and dense if $|E|$ is close to $|V|$. \\\n",
    "And so if $|E|$ differs an order of magnitude from $|V|$ the graph is considered sparse, otherwise it is dense.\n",
    "The density degree expression is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{|V|}{|E|(|E| - 1)} \\approx \\frac{|V|}{|E|^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44c212be-1f90-4d00-b0cd-a1f1f2bd3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a graph as a dict of dicts\n",
    "# Output: a boolean True/False\n",
    "\n",
    "def directed(dict_graph):\n",
    "    direct = False\n",
    "    # Iterating through the dictionary items\n",
    "    for node, neighbours in dict_graph.items():\n",
    "        # For each list of adjacency of node\n",
    "        for neighbour in list(neighbours.keys()):\n",
    "            # If the node is not present in the adj list of his neighbours --> break\n",
    "            if node not in list(dict_graph[neighbour].keys()):\n",
    "                direct = True\n",
    "                break\n",
    "    return direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec0d17b4-3d64-47ba-aecb-85220bc15ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a graph as a dict of dicts\n",
    "# Output: number of nodes in the graph\n",
    "\n",
    "def n_users(dict_graph):\n",
    "    # Return number of keys\n",
    "    return len(dict_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2821b314-94dc-4cf2-8e15-8eee0e212945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a graph as a dict of dicts\n",
    "# Output: number of edges in the graph\n",
    "\n",
    "# For this implementation we considered the \"out-degree\" = number of edges coming out from the node \n",
    "def n_interactions(dict_graph):\n",
    "    # Sum the length of lists of edges\n",
    "    n_int = 0\n",
    "    for neighbour in dict_graph.values():\n",
    "        n_int += len(list(neighbour.keys()))\n",
    "    return n_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6de77e33-28fe-4fdc-a516-bb9c27d1a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a graph as a dict of dicts\n",
    "# Output: average number of links for nodes\n",
    "\n",
    "def average_n_links(dict_graph):\n",
    "    return n_interactions(dict_graph) / n_users(dict_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e876a7e4-104c-4f63-b848-11d37ea7fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_OverallFeatures(dict_graph):\n",
    "    \"\"\"\n",
    "    Input: One of the 3 graphs in dictionary format\n",
    "\n",
    "    Output:\n",
    "      Whether the graph is directed or not\n",
    "      Number of users\n",
    "      Number of answers/comments\n",
    "      Average number of links per user\n",
    "      Density degree of the graph\n",
    "      Whether the graph is sparse or dense\n",
    "    \"\"\"\n",
    "    \n",
    "    # quering if the graph is directed\n",
    "    Directed = directed(dict_graph)\n",
    "    \n",
    "    # using a variable to print the output\n",
    "    if Directed: \n",
    "        IsDirectedPrint = 'directed'\n",
    "    else:\n",
    "        IsDirectedPrint = 'undirected'\n",
    "        \n",
    "    # computing the number of users\n",
    "    NofUsers = n_users(dict_graph) \n",
    "    \n",
    "    # computing the number of interactions\n",
    "    NofInteractions = n_interactions(dict_graph)\n",
    "    \n",
    "    # computing the avarege number of links per user\n",
    "    AvgUserLinks = average_n_links(dict_graph)\n",
    "    \n",
    "    # computing density degree\n",
    "    DensityDegree = NofInteractions / NofUsers**2\n",
    "    \n",
    "    # evaluating sparsity/density\n",
    "    if NofUsers / NofInteractions**2 < 10:\n",
    "        SparseDense = 'sparse'\n",
    "    else:\n",
    "        SparseDense = 'dense'\n",
    "        \n",
    "    # print the results\n",
    "    print(f\"The input graph is -> {IsDirectedPrint}\")\n",
    "    print(f\"Number of users -> {NofUsers}\")\n",
    "    print(f\"Number of answers/comments -> {NofInteractions}\")\n",
    "    print(f\"Average number of links per user -> {AvgUserLinks:.2}\")\n",
    "    print(f\"Density degree -> {DensityDegree: .2}\")\n",
    "    print(f\"The graph is -> {SparseDense}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fd84ba2-5d0e-4890-a02a-5d8fa389a606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input graph is -> directed\n",
      "Number of users -> 4668\n",
      "Number of answers/comments -> 10758\n",
      "Average number of links per user -> 2.3\n",
      "Density degree ->  0.00049\n",
      "The graph is -> sparse\n"
     ]
    }
   ],
   "source": [
    "F1_OverallFeatures(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c082fc5-e279-4ed4-aa69-ef9435efe06e",
   "metadata": {},
   "source": [
    "### Functionality 2 - Find the best users!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114fc890-f318-422f-9cd0-7d649dd46a6e",
   "metadata": {},
   "source": [
    "#### Degree centrality\n",
    "As defined in *Introduction to Graph Concepts for Data Science of Aris Anagnostopoulos*, the normalized degree centrality of node $v$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d_v}{|V|-1} \\approx \\frac{d_v}{|V|}\n",
    "\\end{equation}\n",
    "\n",
    "with $d_v$ the degree of the node $v$, i.e. the number of edges incident to the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c83582-ab3e-47e1-864f-124be4f1bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the degree as \"out-degree\"\n",
    "def degree(dict_graph, node):\n",
    "    return len(list(dict_graph[node].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffc2ca18-61b9-4fe9-afe5-fbaa9b18aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the degree as \"in-degree\"\n",
    "def in_degree(dict_graph, node):\n",
    "    out = 0\n",
    "    for u in list(dict_graph.keys()):\n",
    "        if u != node and (node in list(dict_graph[u].keys())):\n",
    "            out += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83655f9a-1eea-4b88-96e6-802b18206f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the definition\n",
    "def degree_centrality(dict_graph, node):\n",
    "    out = degree(dict_graph, node) / n_interactions(dict_graph)\n",
    "    print(f\"{out:.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b48406a-a9ec-4841-971a-5cd472374042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018\n"
     ]
    }
   ],
   "source": [
    "degree_centrality(d1, \"30155\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e52b09d0-6513-40ae-9d13-0dddf34cc31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3e-05\n"
     ]
    }
   ],
   "source": [
    "degree_centrality(d1, \"4120\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45da09-30c8-4288-b7d4-0744200af52a",
   "metadata": {},
   "source": [
    "#### Closeness centrality\n",
    "As defined in *Introduction to Graph Concepts for Data Science of Aris Anagnostopoulos*, the normalized closeness centrality of node $v$ is:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{|V|-1}{\\sum_{u \\epsilon V} d(v,u)} \\approx \\frac{|V|}{\\sum_{u \\epsilon V} d(v,u)}\n",
    "\\end{equation}\n",
    "\n",
    "with $d(v,u)$ the distance between nodes $v$ and $u$, that is the length of a shortest path between $v$ and $u$.\\\n",
    "As a design choice $d(v,u)$ is taken as the inverse of the weight of the edge of $(v,u)$, this leads to an inverse relationship between interaction and distances: the more a user posts, the closer he gets to the comunity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "777470dc-f01d-45cf-8f23-a38c6242e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outgoing_edges(dict_graph, node):\n",
    "    return list(dict_graph[node].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c16d0278-6ace-4ba3-989d-f2d3b103866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a graph as a dict of dicts, the start node\n",
    "# Output: two dictionaries: the first memorize the nodes involved in the shortest paths, the second is the shortest path in distance terms.\n",
    "# Pseudocode from: https://en.wikipedia.org/wiki/Dijkstra's_algorithm#Pseudocode\n",
    "\n",
    "def shortest_path_Dijkstra(dict_graph, node):\n",
    "    \n",
    "    # Initialize two dictionaries to track visited nodes and weights\n",
    "    nodes_dist = dict.fromkeys(dict_graph.keys(), float('inf'))\n",
    "    \n",
    "    # Initialize another dictionary that track the nodes in the shortest path\n",
    "    previous_nodes = dict.fromkeys(dict_graph.keys())\n",
    "    \n",
    "    # Create a list of the unvisited nodes \n",
    "    unvisited = list(dict_graph.keys())\n",
    "    \n",
    "    # First update on the selected node\n",
    "    nodes_dist[node] = 0\n",
    "    \n",
    "    # Start iterating until each node is visited\n",
    "    while(unvisited):\n",
    "        \n",
    "        # Select the current min_node with a for-loop\n",
    "        min_node = None\n",
    "        for n in unvisited:\n",
    "            if min_node == None:\n",
    "                min_node = n\n",
    "            elif nodes_dist[n] < nodes_dist[min_node]:\n",
    "                min_node = n\n",
    "            \n",
    "        # Update distances of the current min_node's neighbors\n",
    "        neighbors = get_outgoing_edges(dict_graph, min_node)\n",
    "        for neighbor in neighbors:\n",
    "            # Calculate the possible update to the shortest path and check if it is < then the actual distance\n",
    "            possible_update = nodes_dist[min_node] + dict_graph[min_node][neighbor]['weight']\n",
    "            if possible_update < nodes_dist[neighbor]:\n",
    "                nodes_dist[neighbor] = possible_update\n",
    "                # We also update the best path to the current node\n",
    "                previous_nodes[neighbor] = min_node\n",
    "        \n",
    "        # Remove the current min_node from the unvisited list\n",
    "        unvisited.remove(min_node)\n",
    "    \n",
    "    return previous_nodes, nodes_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58f8853f-b26c-4e29-9c96-739d4fd8c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_shortest_path(previous_nodes, start_node, target_node):\n",
    "    path = []\n",
    "    node = target_node\n",
    "    \n",
    "    while node != start_node:\n",
    "        if node == None:\n",
    "            return []\n",
    "        path.append(node)\n",
    "        node = previous_nodes[node]\n",
    "        \n",
    "    # Add the start node manually\n",
    "    path.append(start_node)\n",
    "    \n",
    "    return list(reversed(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a63c496-a303-4f36-a0fb-dabe51f48598",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehsi , calcl = shortest_path_Dijkstra(d_merge, '18313')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e403306-33c0-443a-abb7-9f378f5ea1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peso totale con cui vado da '18313' a \"19964\"\n",
    "min_dis_dijk = calcl[\"19964\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa3e46eb-578a-46f6-b214-dbe145d1c273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18313', '28258', '6782', '19964']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path completo dall'inizio alla fine\n",
    "nodes_shortest_path(ehsi,'18313', \"19964\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e433537-64c2-4ca1-b45b-38a157bdea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_closeness_centrality(dict_graph, v):\n",
    "    \n",
    "    # Create a list with all the nodes except the one considered\n",
    "    nodes = list(dict_graph.keys())\n",
    "    nodes.remove(v)\n",
    "    \n",
    "    # Iterate and calculate the denominator adding all the distances calculated with Dijkstra\n",
    "    denom = 0\n",
    "    _, dists = shortest_path_Dijkstra(dict_graph, v)\n",
    "    \n",
    "    for u in nodes:\n",
    "        denom += dists[u]\n",
    "        \n",
    "    # Return the result\n",
    "    return (n_users(dict_graph)) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b0601fd-bf3c-4ec6-819c-b915421bf3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_closeness_centrality(d_merge, \"18313\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fb2bb46-1c44-4d6a-9315-d82a2812a78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_closeness_centrality(d_merge, \"19964\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df06f29-b1a6-496e-8dbb-6d1149e30120",
   "metadata": {},
   "source": [
    "#### Page rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc2d6ff-a139-4cda-8323-9ec597906d4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "The page rank algorithm will be implemented with Random Surfer Model in which we will use a value $\\alpha$ to adjust all the points in the matrix of edges(a matrix where there are ones if the edge exists and zeros otherwise).\n",
    "The matrix of edges could be full of zeros, so in order to save space in memory, we will not store it. We simply modify our dictionary graph, changing the weights and replacing each with:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\alpha}{|edges(v)|}+ (1-\\alpha)\\cdot w_{i}\n",
    "\\end{equation}\n",
    "\n",
    "where $n$ is the node key in the dictionary, so $v \\in V$ and $w_{i}$ is the weight of the $i-th$ edge.\n",
    "\n",
    "For each other node where there is a zero in our \"ideal\" matrix, we will replace it simply with:\n",
    "\\begin{equation}\n",
    "\\frac{\\alpha}{|edges(v)|}\n",
    "\\end{equation}\n",
    "\n",
    "But of course it's not necessary to add it to the dictionary, since we write only the existing edges. We will assume that if an edge doesn't exist, the probability to go random to another not necessary connected node is $\\frac{\\alpha}{|edges(v)|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3d47064-91dd-4f7b-9d98-51b79c1984d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parents(dict_graph, node):\n",
    "    out = []\n",
    "    for u in list(dict_graph.keys()):\n",
    "            if u != node and (node in list(dict_graph[u].keys())):\n",
    "                out.append(u)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07dbab77-b9ce-490b-9b71-db72228243bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_children(dict_graph, node):\n",
    "    return list(dict_graph[node].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "99f7bdb2-3271-47f0-b717-7313148b94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_dict = dict.fromkeys(d_merge.keys(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2c32991c-83e9-42fe-b4f0-cb4d08965a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pagerank(dict_graph, node, alpha, pagerank):\n",
    "    parents = get_parents(dict_graph, node)\n",
    "    pagerank_sum = sum((pagerank[node] / len(get_children(dict_graph, node))) for node in parents)\n",
    "    random_walk = alpha / n_users(dict_graph)\n",
    "    pagerank[node] = random_walk + (1 - alpha) * pagerank_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "27467827-e1ec-49e3-92fb-8face013ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PageRank_one_iter(dict_graph, alpha, pagerank):\n",
    "    nodes = list(dict_graph.keys())\n",
    "    for node in tqdm(nodes):\n",
    "        update_pagerank(dict_graph, node, alpha, pagerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "70004ef9-ef61-4945-9718-9a8c2071aeaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alpha = 0,85 is optimal value for the damping factor\n",
    "def page_rank(dict_graph, pagerank, alpha=0.85):\n",
    "    PageRank_one_iter(dict_graph, alpha, pagerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a4f521ae-1298-426f-a50c-7f9b38135dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5134/5134 [00:14<00:00, 343.83it/s]\n",
      "100%|██████████| 5134/5134 [00:15<00:00, 324.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for --> 1526 values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5134/5134 [00:15<00:00, 321.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for --> 1600 values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5134/5134 [00:16<00:00, 312.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for --> 1604 values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5134/5134 [00:15<00:00, 321.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for --> 1604 values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5134/5134 [00:15<00:00, 322.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for --> 1604 values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5134/5134 [00:17<00:00, 295.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for --> 1606 values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5134/5134 [00:16<00:00, 305.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for --> 1617 values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5134/5134 [00:16<00:00, 320.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for --> 1639 values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    if i == 1:\n",
    "        page_rank(d_merge, pagerank_dict)\n",
    "        prev = list(pagerank_dict.values())\n",
    "    if i > 1:\n",
    "        page_rank(d_merge, pagerank_dict)\n",
    "        cont = 0\n",
    "        for pr, actual in zip(prev, list(pagerank_dict.values())):\n",
    "            if pr == actual:\n",
    "                cont += 1\n",
    "        print(\"Convergence for --> \" + str(cont) + \" values.\")\n",
    "        prev = list(pagerank_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464eae7-5352-4065-841f-c29e847a46b5",
   "metadata": {},
   "source": [
    "#### Betweeness\n",
    "\n",
    "As defined in *Introduction to Graph Concepts for Data Science of Aris Anagnostopoulos*, the Betweeness centrality of node $v$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{u, w \\epsilon V \\backslash \\{v\\}} \\frac{g_{u w}^v}{g_{uw}} \\frac{2}{|V|^2 - 3|V| + 2} \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "with $g_{uw}$ the shortest path that connects nodes $u$ and $w$ and $g_{u w}^v$ the set of those shortest paths between u and w that contain node $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6994164c-5930-4aea-946e-dc1ff263fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is a recursive implementation of BFS that finds all the paths between a start and an end node, but it will return only the ones that are minimum paths.\n",
    " \n",
    "def find_all_shortest_paths(dict_graph, start_node, end_node, dijkstra_dist, path = []):\n",
    "    \n",
    "    # Initialize the list to do initial checks\n",
    "    path = []\n",
    "    \n",
    "    # Initialize the total weight\n",
    "    tot_weight = 0\n",
    "    \n",
    "    # Add to the path the starting node (after that in the recursion this list is used to track all the visited nodes)\n",
    "    path = path + [start_node]\n",
    "    \n",
    "    # If the start is equal to the end\n",
    "    if start_node == end_node:\n",
    "        return [path]\n",
    "    \n",
    "    # If the node is not in the graph, we return an empty list\n",
    "    if start_node not in dict_graph:\n",
    "        return []\n",
    "    \n",
    "    # Create the final list that contains all the paths\n",
    "    shortest_paths = []\n",
    "    \n",
    "    # Iterate over the neighbours of the start-node  \n",
    "    for node in dict_graph[start_node]:\n",
    "        # If the node is not in the path\n",
    "        if node not in path:\n",
    "            # Recursive call the function to apply BFS\n",
    "            newpaths = find_all_shortest_paths(dict_graph, node, end_node, path)\n",
    "            # Iterating over the lists of paths founded\n",
    "            for newpath in newpaths:\n",
    "                # Check for each node of this path if it is a minimum path, based on the distance calculated by dijkstra\n",
    "                for i in range(0, len(newpath)-1): \n",
    "                    tot_weight += d[newpath[i]][newpath[i+1]][\"weight\"]\n",
    "                # If this path is a mimum path, then append to the final list of lists\n",
    "                if tot_weight == dijkstra_dist:\n",
    "                    shortest_paths.append(newpath)\n",
    "                    # Re-initialize the total weight\n",
    "                    tot_weight = 0\n",
    "    return shortest_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "e51e9c76-1dc1-462d-aa22-2f503cd3c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betweeness(dict_graph, v):\n",
    "    \n",
    "    # Create a list with all the nodes except the one considered\n",
    "    nodes = list(dict_graph.keys())\n",
    "    nodes.remove(v)\n",
    "    \n",
    "    const = (2 / ((n_users(dict_graph))**2 - 3*(n_users(dict_graph)) + 2))\n",
    "    result = 0\n",
    "    \n",
    "    # Iterating the list with a double for-loop\n",
    "    for u, w in tqdm(zip(nodes, nodes)):\n",
    "            g_u_v_w = 0\n",
    "            g_u_w = 0\n",
    "            previous_nodes, nodes_dist = shortest_path_Dijkstra(dict_graph, u)\n",
    "            path = nodes_shortest_path(previous_nodes, u, w)\n",
    "            # If the shortest path between u and w exists\n",
    "            if nodes_dist[w] != float('inf') and nodes_dist[w] != 0 :\n",
    "                g_u_w = nodes_dist[w]\n",
    "                # And if exists and node v is in the path...\n",
    "                if v in path:\n",
    "                    g_u_v_w += 1\n",
    "                result += (g_u_v_w / g_u_w) * const\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69871f-b944-49af-b996-9cc1e1524049",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = find_all_shortest_paths(d_merge, \"18313\", \"19964\", min_dis_dijk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "14dad17c-1f45-4ff8-879c-cb0d01852919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200\n",
      "5200\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getrecursionlimit())\n",
    "sys.setrecursionlimit(5200)\n",
    "print(sys.getrecursionlimit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8e7ca68d-c2a8-4653-bd7f-42fa279ee7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008620370926482761"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova = nx.betweenness_centrality(merged)\n",
    "prova[\"18313\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fb6e4-12fb-4304-b97c-c6dac18fa1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F2_BestUser(Node, TimeInterval, Metric, Graph = MergedGraphs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    A user/node\n",
    "    A tuple of two dates in ISO format representing an interval of time, e.g. (2015-12-04, 2016-12-04) \n",
    "    An integer corresponding to the following metrics: \n",
    "      1 -> Betweeness \n",
    "      2 -> PageRank\n",
    "      3 -> ClosenessCentrality \n",
    "      4 -> DegreeCentrality\n",
    "    The graph on which to perform the analysis.\n",
    "\n",
    "    Output:\n",
    "    The value of the given metric applied over the complete graph for the given interval of time.\n",
    "    \"\"\"\n",
    "\n",
    "    TimeInterval = tuple(map(dt.fromisoformat, TimeInterval)) # converting time interval into datetime format\n",
    "    IntToMetric = {1: Betweeness, 2: PageRank, 3: ClosenessCentrality, 4: DegreeCentrality} # dictionary associating the\n",
    "                                                                                           # input integer to the\n",
    "                                                                                           # corresponding metric function\n",
    "\n",
    "    TimeIntervalGraph = Graph.copy() # creating a copy on which to perform the time filtering\n",
    "    TimeIntervalGraph.remove_edges_from([(n1, n2) for n1, n2, time in TimeIntervalGraph.edges(data=\"time\") \n",
    "                                        if (time >= TimeInterval[0] & time <= TimeInterval[1])]) # removing the edges that not belong\n",
    "                                                                                                 # to the input time interval  \n",
    "\n",
    "    return IntToMetric[Metric](TimeIntervalGraph, Node)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
